\chapter{Background}
\label{background}

\section{Software evolution}
The analogy of the term \emph{evolution }\rm in the field of software
engineering was first used by \citet{lehman} in his laws of software evolution.
Software does not evolve by intrinsic feedback loops like evolution in plants
and animals, but by extrinsic feedback that comes from the operational domain.

\paragraph{}
\citeauthor{lehman} identified eight laws that are the driving factors of
changes in software systems. These laws can be roughly categorised into two
categories: laws related to the product (e.g., source code, and internal
quality), and laws related to the process (e.g., organisation, development
process, business requirements and user satisfaction).

\paragraph{}
The laws related to the software process tell us that in order for a software
system to stay useful and satisfactory, it needs to keep changing according to
users' needs. What needs to change is fed through the feedback system.

On average and in terms of activity, the organisation of a software development
process does not change much during a product's life time; \citeauthor{lehman}'s
fourth law: Conservation of Organisational Stability.

\paragraph{}
The laws related to the software product tell us that as the source code of the
software system changes, its complexity will increase and its quality will
decline unless proper actions are taken to maintain it or reduce the complexity
and improve its quality. However, the contents of the product will be
statistically invariant during the active life time of a product.

\paragraph{}
\citeauthor{lehman}'s laws of software evolution are valid for both closed
source software and open-source software. These laws form the foundation of
what we understand as software evolution.

\paragraph{}
The evolution of a software system comprises various measures on different
moments in time. For instance, the lines of code metric over a project's life
time is a way to measure the evolution of lines of code in a software project.



\section{Project survivability}
%% perhaps restructure in Project success, and Project survivability %%
In a study by \citet{samoladas2010} a method for survival analysis on OSS
projects was proposed. The authors used duration data regarding Free/Libre
Open-Source Software (FLOSS) projects to predict the survivability of the
projects by examining their duration, combined with other characterstics such
as application domain and number of committers. These metrics give insight in
the survivability chances of a project. It was also found that adding a
developer to the team of contributors increased the survivability of the
project substantially.

The authors proposed two main research issues to be addressed in the future.
The first one is to add more projects to the study with possibly a different
categorisation. And second, the effects of more project parameters, such as
programming language should be examined. This is not trivial since typically
more than one language is used in each project.

\paragraph{}
A study by \citet{raja2012} on defining a measure of OSS project survivability.
They have been looking for vitality of OSS projects: the ability of a project
to grow and maintain its structure in the presence of perturbations. They
identified three dimensions of project viability: vigor -- the ability of a
project to grow --, resilience -- the ability of a project to recover from
disturbances --, and organisation -- the structure exhibited in the project.
These dimensions represent three distinct characteristics of project viability.

This measure is of use to determine survivability of a project, however, it is
a snapshot of a single point in time. It does not take into account the events
prior to this point in time, nor does it enable prediction of survivability in
the near future. Therefore, it can be challenging to get an objective view on a
project's survivability as a representative point in time has to be selected.

\paragraph{}
\citet{crowston2003} identified measures that can be applied to assess the
success of OSS projects. The authors used the ``Model of Information Systems
Success'' by \citet{delone1992} to evaluate OSS project success. The aspects
identified by \citeauthor{delone1992} are elaborated; output of systems
development -- it is believed that a project that has a high frequency of
releases is healthy --, process of systems development -- the number of
developers, the individual level of activity, and cycle time (time between
releases) --, and project effects -- employment opportunities of the
contributors, individual reputation, and knowledge creation. In this study it
was found that many of these aspects are indicators of OSS project success.

Although the cycle time is an aspect that could be measured automatically, the
other aspects such as employment opportunities, reputation, and knowledge
creation are very hard to get into numbers and therefore hard to automate.

\paragraph{}
Another study conducted by \citet{crowston2006} extends the previous study by
using Free/Libre Open-Source Software (FLOSS) projects. In addition to what was
found in the previous study, they had found that the number of developers as a
simple count of developers is a flawed number as it aggregates the number of
developers leaving and the number of developers joining a project. A 'churn' of
the developers or a 'tenure' of individuals would be more appropriate.

\paragraph{}
A study conducted by \citet{wang2012} has shown that warning signs can be found
in six crucial factors of OSS projects success: developer participation effort,
developer service quality, software license restrictiveness, targeted users,
community social network ties, and community quality of social ties.

\paragraph{}
\citet{karus2013} explored a method known as \emph{wavelet analysis }\rm to
analyse software evolution data. The wavelet analysis interprets evolution data
as a series of signals and is able to find sequences in this signal. The
sequences of multiple projects can be compared in order to find recurring
patterns. Karus was able to detect 998 similar patterns across 27 OSS projects.
He concluded that wavelet analysis can be a powerful tool for identifying
evolutionary events.





\section{Wavelet analysis}
\label{wavelet_analysis}
Wavelets are functions that satisfy certain mathematical requirements and are
used in representing data or other functions \cite{graps}.

\subsection{Waveforms and wavelets}
A waveform, or wave, is a signal (time-series) in a two-dimensional space.
Waveforms are used to model many types of signals, such as audio,
electromagnetic (light), gravitational, and quantum mechanical waves.

\paragraph{}
In many models of waveforms the two dimensions typically represent the time and
frequency domains. An audio signal is an example of a waveform modeled that way.

In visualisations of waveforms, the time domain is often plotted on the
horizontal axis, and the frequency domain on the vertical axis. In the example
of an audio wave (see Figure \ref{figure:wave}), the frequency domain may
represent the amplitude, or the frequency, and the time the duration of the
amplitude or frequency value.

\input{figures/wave}

However, the time domain is not specifically bound to time intervals only. A
less intuitive approach could be to model frequency in the time domain. That
way the frequency relation to amplitude of a signal can be analysed, but the
time information is lost.

\subsection{Discrete wavelet transform}
To be able to analyse and compare wavelets of different lengths and scales, a
way to transform the signal is needed. Discrete wavelet transformation is the
operation of applying a filter function, or set of filter functions (also known
as 'filter bank') to the wavelet. 'Discrete' because the signal is segmented
into samples; which number is possibly finite, possibly infinite. The
discrete wavelet transformation is a way of sampling the signal at different
intervals giving a natural means of scaling the signal \cite{karus2013}.

\paragraph{}
Wavelet analysis is similar to Fourier analysis with the difference that
wavelet transform deals with time and frequency information, and Fourier
transform deals with frequency information only. Wavelet analysis has the
advantage over Fourier analysis that it can deal with non-smooth ('choppy')
signals, as opposed to the limitation of only sines and cosines \cite{graps}.

\paragraph{}
In digital signal processing, the category of methods that discrete wavelet
transform belongs to, there is a phenomenon called the \emph{uncertainty
principle}\rm. It means that the shorter the signal, the harder it is to
identify the signal. On the contrary, the longer the signal, the higher the
accuracy in identifying the signal. However, it is impossible to identify a
signal with 100\% accuracy as there is always a degree of uncertainty in
identifying a signal. This is a fundamental principle in digital signal
processing and it highly influences the ways the method can be used.

High-frequency signals vary quickly and are shorter in nature than
low-frequency signals that vary slowly. Therefore, in order to identify
high-frequency signals, time resolution is needed, and to identify
low-frequency signals, frequency resolution is needed. It is not possible to
zoom in on both parts of the signal at the same time. Zooming into time will
compromise on frequency and vice versa.

\paragraph{}
Wavelet transform is the transformation of signals (time-series) by
decomposing the signal into wavelet/shift coefficients, and scaling/filter
coefficients based on filter functions (i.e., filters) \cite{karus2013}.

Shifting/translating is the operation of moving the wavelet in the time domain
and filtering/dilating is the operation of scaling the wavelet in the frequency
domain. These operations are illustrated in Figure \ref{figure:shifting} and
Figure \ref{figure:filtering} respectively.

\input{figures/shifting}
\vspace{1em}
\input{figures/filtering}

\paragraph{}
Wavelet transform has proven important in signal processing thanks to its
inherent properties which allow comparisons at different scales and shifts
\cite{karus2013}. Compared to other time series analysis techniques, the main
advantages of wavelet transformations in the analysis of signals are:
\begin{itemize}
	\item Wavelet/shift coefficients allow fuzzy matching as differences in details
	are 'smoothed out'.
	\item Filter/scale coefficients allow detection of small anomalies in series.
	\item Decomposition levels make series of different lengths or scale
	comparable.
\end{itemize}

\subsection{Wavelet functions}
A wavelet function is a function that defines a wavelet \cite{wadkar}. Many
wavelet functions exist and differ largely in complexity and applicability
depending on the signal to be processed.

A wavelet can be defined in the following ways, given that $T$ is the set of
time values of the signal, and $F$ the set of frequency values of the signal 
\cite{graps}.
\begin{description}
	\item[Wavelet function (mother wavelet)] \hfill \\ $\Psi: T \rightarrow
	F$\\ $\Psi(t) = f$, such that $t \in T$ and $f \in F$.\\
	A bijective function mapping $T$ onto $F$ and producing the shape of the
	wavelet.

	\item[Scaling function (father wavelet)] \hfill \\ $\Phi: F \rightarrow
	T$\\ $\Phi(f) = t$, such that $t \in T$ and $f \in F$.\\
	A bijective function mapping $F$ onto $T$ and producing the scale of the
	wavelet.

	\item[Scaling filter] \hfill \\ A low-pass filter of length $2N$ and sum $1$.
	A high-pass filter can be calculated as the quadrature mirror filter of the
	low-pass filter. All Daubechies wavelets can be defined by the scaling filter.
\end{description}

\subsection{Haar wavelet}
The Haar wavelet is a member of the Daubechies family of wavelets, based on the
work of the Belgian mathematician Ingrid Daubechies \cite{graps}. The
Daubechies wavelets is a family of orthogonal wavelets defining a discrete
wavelet transform. All wavelets of the Daubechies family can be entirely
defined by their scaling filter.

The Haar wavelet is a Daubechies wavelet and has a filter length of 2 and
is therefore also referred to as the Daubechies-2 (D2) filter. It is the
simplest wavelet of the Daubechies family and is defined by the mother wavelet
function as shown in Figure \ref{figure:haar} \cite{stankovic}.

\input{figures/haar}

\paragraph{}
In 1910, the Hungarian mathematician Alfred Haar introduced Haar functions. The
Haar transform is one of the earliest examples of what is known now as a
compact\footnote{Compact support: the ability of a function to be scaled
smoothly.}, dyadic\footnote{A dyadic function scales in powers of 2.},
orthonormal\footnote{Orthonormal: domain and range are orthogonal and of
equal length.} wavelet transform \cite{stankovic}. The Haar function is the
simplest and oldest orthonormal wavelet with compact support.

\subsection{Discrete Haar transform}
The Haar filter function varies in scale by splitting the input signal using
different scale sizes.

\paragraph{}
Having a signal over the domain from 0 to 1, the Haar transform divides the
signal into two wavelets that range from 0 to $1/2$ and from $1/2$ to 1. Then
the division can be repeated giving four wavelets that range from 0 to $1/4$,
from $1/4$ to $2/4$, from $2/4$ to $3/4$, and from $3/4$ to 1 \cite{graps}.

\paragraph{}
The original signal is decomposed successfully into components of lower
resolution. The decomposition can be repeated as long as the resolution of
the original signal allows. In the case of the Haar filter this means as long
as the number of resulting coefficients is larger than 2 (i.e., the filter
length).

\paragraph{}
In each scaling/filtering step (i.e., decomposition) the Haar function adds
more detail to the wavelets in the current level. The Haar filter captures the
differences between scale levels. These resulting coefficients can be used to
compare signals regardless of scale or length \cite{graps}.

Wavelet transform using the Haar filter is illustrated in Figure
\ref{figure:haar_transform}. The figure shows a stacked plot of a signal
(lower most graph) and the transformed signals at each level of decomposition.

\input{figures/haar_transform}

The spikes shown in Figure \ref{figure:haar_transform} represent the
coefficients found at the relevant level. The number of coefficients increases
by a factor of 2 at each level. In the figure, the original signal consists of
64 samples, therefore, the signal can be decomposed to a maximum of $64 \log_{2}
= 6$ levels; having 

\paragraph{}
The coefficients should only be compared to coefficients of the same type. The
scale/filter coefficients are incomparable to the wavelet/shift coefficients,
because that would mean comparing time with frequency which leads to invalid
relations between signals.

\paragraph{}
Wavelet transform using the Haar filter is widely used in other fields. For
example, as a way of digitalising an analogue signal in A-D converters, pattern
recognition in image and video processing, face recognition, image processing,
data coding, multiplexing, digital filtering, digital speech processing, voice
controlled computing devices, robotics, and compression mechanisms \cite{khan,
stankovic, wadkar}.

\begin{comment}
- Literature study
- Software evolution
- Project success
- Project survivability

This chapter contains all the information needed to put the thesis into
context. It is common to use (a revised version) of your literature survey for
this purpose.
It is important to refer from your text to sources you have used, as listed in
your bibliography section (appendix). For example, “XP is a recent agile
development method [1]” is a common style of doing this, where the following
entry would be included in your bibliography:
[1] K. Beck, E. Gamma, Test infected: Programmers love writing tests, Java
Report 3 (7) (1998) 51–56.
If you want to refer to books you have read as part of the curriculum, you can
also do so in this way.
Have a look at Chapter 2 of this example thesis at Paul’s
homepage\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/RichardKettelerij.pdf}.
\end{comment}