\chapter{Research}
\label{research}

This chapter describes the execution of the research.

\section{Data set}
The data set of 250 projects was selected using the method described in section
\ref{method:data}. It was conducted in the following steps:

\begin{description}
	\item[1. Gathering, initial validation and cleansing] \hfill \\ The initial
	data of 12,360 projects was gathered from Ohloh.net in July 2013. then
	validated and cleansed accordingly by Magiel Bruntink using the OhlohAnalytics
	tool \cite{ohlohanalytics, bruntink2014}. The result of this step is a
	consistent data set of evolution data of 10,811 projects.

	\item[2. Fraction of evolution] \hfill \\ The evolution data of the
	projects contained gaps. Many projects did not have a continuous series of
	evolution data. We expect a continuous series of data to be required in order
	to analyse time series for a project. Therefore, a number representing the
	continuity of the data was needed. For each project the difference between the
	minimum and maximum values of the $age\_in\_months$ fact is taken, added by
	one, giving us the expected number of data points a project should have. The
	calculation of the fraction of total evolution data is done for each project.
	
	Now that we know the fractions of the total evolution data for each project, we
	can filter these projects and keep only the projects that have all data points
	between minimum and maximum $age\_in\_months$. From the set of 10,811
	projects, a total number of 6,418 projects is left.
	
	\item[3. Minimum evolution length] \hfill \\ As a final step of validating and
	selecting the master data set, we will be keeping only the projects that have
	at least 12 data points (i.e., have an evolution period of at least 1 year).
	
	After this selection, a set of 5,986 projects is left of where the sample can
	be taken from.
	
	\item[4. Sample selection] \hfill \\ For the selection of a sample of 250
	projects, we use the tool created by Nagappan \emph{et al. }\rm\cite{nagappan}.
	This tool takes a sample set and selects additional projects to add to the
	sample that increase the overall representativeness of that sample.
	
	The basis for the sample selection is an atomic set containing the
	project ``Mozilla Firefox''. The choice for this project as basis is done
	because of its popularity and size.
	
	The analysis in step 1 revealed that this project has many defficiencies in
	its evolution data available at Ohloh.net. However, after all steps of
	validation, cleansing, and selection, it is still included in the 5,986
	projects. A quick view on the final data of ``Mozilla Firefox'' shows that it
	contains 61 subsequent data points. Therefore, no objective arguments can be
	made not to use it as the basis of the sample.
	
	The tool then iteratively selects 249 more projects and adds it to the sample.
	Each addition project is selected by its score to maximally increase the
	representativeness of the sample. The resulting set of 250 projects was scored
	to be 99.5\% representative to the master data set from Ohloh.net.
\end{description}

\begin{comment}
\begin{table}
\centering
	\caption{Evolution signals}
	\begin{tabular}{| p{4cm} | p{4cm} | p{4cm} |}
	\hline
	\bfseries{Time domain}\rm & \bfseries{Frequency domain}\rm &
	\bfseries{Description}\rm
	\\
	\hline Age in months	& LOC & LOC per month. \\
							& Code Churn & Churn per month. \\
							& Active Developers & Contributors per month. \\ \hline
	\end{tabular}
\label{tab:series}
\end{table}

\section{Analysis}
The analysis is done in the following steps:
\begin{enumerate}
	\item Wavelet transform.
	\item Similar sequence identification.
	\item Similar sequence grouping.
\end{enumerate}

\subsection{Wavelet transform}
During the wavelet transform steps, each project is analyzed separately using
each combination as specified in table \ref{tab:series}.

The data is prepared by sorting it by the time domain, indexing, and aggregate
data where appropriate. Then the discrete wavelet transform is applied using the
Haar filter.

The resulting data structure is saved in a file representing the transform data
for a specific project. A file per coefficient (scale and filter) is saved
with aggregated data for all projects. These files contain all values per level
found for all projects.

\subsection{Similar sequence identification}
The results of the wavelet transforms are analyzed and identified as sequences.
A sequence is a series of values with a minimum length of 3 values and a maximum
length of 65. The sequences of each project are compared to the sequences of
every other project. In case a similar sequence is found, it is recorded.
Similar is having the same series of values with a maximum deviation of
0.005.

All found similar sequences are saved in a file per coefficient.

\subsection{Similar sequence grouping}
The similar sequence grouping step takes the similar sequences found in the
previous step and classifies the sequences by number of projects, number of
occurances, lengths, etc. The resulting file enables selection of sequences by
these properties. The resulting data is used for partial manual, and partial
automatic validation.
\end{comment}

\begin{comment}
This chapter reports on the execution of the research method as described in Chapter 3.

If the research has been divided into phases (e.g., using sub questions) the
phases are introduced, reported on and concluded individually. If needed this
Chapter could be split up to balance out the sizes of all Chapters.
An example Research Chapter is provided as Chapter 3 at Paulâ€™s home
page\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/ReneWiegers.pdf}.
\end{comment}
