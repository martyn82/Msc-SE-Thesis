\chapter{Research}
\label{research}

In this chapter, I describe the research execution. First, the data selection is
described. Then the wavelet transform and analysis is described. Finally, the
classification of patterns.

\section{Data selection}
\label{method:data}
A total number of 250 OSS projects will be used for this study. This number is
somewhat arbitrary, but was chosen to have a larger data set than the initial
study conducted by \citet{karus2013}, as a larger data set yields more
trustworthy results. However, the research still needs to be feasible within
the given time constraints of four months.

Several steps have been taken prior to the selection of this set of 250
projects. The steps are described in the following sections.

\subsection{Data gathering}
The data for this research is gathered from
Ohloh\footnote{\url{http://www.ohloh.net}}: \emph{``Ohloh is a free, public
directory of Free and Open Source Software and the contributors who create and
maintain it.'' }\rm \cite{ohloh}. At the time of this writing, Ohloh tracks more
than 660,000 OSS projects varying in all ranges of size, and popularity. The
most popular projects currently being Apache HTTP Server, Apache OpenOffice,
Apache Subversion, Bash, Firebug, Linux Kernel, Mozilla Firefox, MySQL, PHP,
and Ubuntu.

\paragraph{}
For this research, the data provided by a tool by \citet{ohlohanalytics} is
used. This tool, ``\emph{OhlohAnalytics}\rm'', was developed as part of the
replicative research by \citet{bruntink2014}, and provides a validated and
cleansed data set of 12,360 OSS projects, collected from Ohloh in July 2013.

\subsubsection{Initial validation and cleansing}
The OhlohAnalytics tool by \citet{ohlohanalytics} did the initial analysis,
validation, and cleansing of the data by detecting inconsistent values and
removing these records from the data set. Additionally, extra data fields are
derived or aggregated from and added to the raw data for convenience.

The complete list of data fields provided by OhlohAnalytics are shown in Table
\ref{table:fields}. The column 'Source' specifies if the field is either 'Raw'
data (i.e., directly available from Ohloh), or if it is added by the
OhlohAnalytics tool as derived from an operation on one or more other fields.
In the latter case, the fields are listed.

The result of the data gathering and cleansing step is a consistent data set of
evolution data of 10,811 projects.

\input{tables/data_fields}

\subsection{Data validation}
\subsubsection{Subsequent data series}
Although the data set should be consistent after validation and cleansing,
another validation step is needed prior to the selection of the 250 projects
for the study.

The evolution data of the 10,811 projects contained gaps. It is expected by
Hypothesis \ref{hyp:subsequent_data} that a subsequent series of data is
required to be able to analyse time series for a project. Therefore, a number
representing the \emph{fraction of continuity }\rm of the evolution data per
project was calculated. For each project the difference between the minimum and
maximum values of the $age\_in\_months$ fact is taken, added by one, giving the
expected number of data points for a project. The computation of the fraction
of total evolution data is done for each project.

After the fractions of the total evolution data for each project were caculated,
the projects were filtered and only those projects that have all data points
between minimum and maximum $age\_in\_months$ are kept. This reduces the total
number of projects to 6,418.

\subsubsection{Minimal sequence length}
A time series of 1 (monthly) data point is not analysable over time and
incomparable to larger projects. The threshold of at least 12 monthly data
points is chosen to minimise noise in the evolution data that may be caused by
too young or unstable projects.

After this selection the data set contains 5,986 projects.

\subsubsection{Sample selection}
For the selection of a sample of 250 projects, the tool created by
\citet{nagappan} at Microsoft Research is used. This tool takes a - possibly
atomic - sample set and selects additional projects to add to the sample that
increase the overall representativeness of that sample. The tool scores
projects by two metrics: total lines of code, and yearly contributors count.

The tool iteratively selects 250 projects and adds it to the sample. Each
additional project is selected by its score to maximally increase the
representativeness of the sample as a whole, compared to the master data.

The master data is a list of 20,028 projects tracked by Ohloh delivered with
this tool. A pre-filtering of this master data was done to make the tool select
only the projects that appear in the data set of 5,986 projects.

\subsection{Dead projects}
In the search to find patterns that are warning signs leading to the end of
code evolution, the projects that satisfy the definition of a dead project (see
also section \ref{def:dead}) were extracted from the 250 projects.

\section{Wavelet transform and analysis}
\subsection{Project signals}
\label{section:signals}
The evolution of the 250 projects is modeled as signals for wavelet transform
and analysis. The constructed project signals are modeled with
$age\_in\_months$ in the time domain, and $LOC$ in the frequency domain. Figure
\ref{figure:signal} illustrates a project's signal.

\input{figures/signal}

\paragraph{}
The rationale for chosing 'lines of code' in time series is done because of
simplicity and intuitive interpretation. LOC is not a way to measure
productivity in a project, but it is sufficient to measure activity.

\paragraph{}
The $LOC$ metric used in this study for the transform and analysis is slightly
different from the $loc\_fact$ that is available at Ohloh. The goal for the
signal processing is to be able to detect patterns in a project's code
activity. This means that all changes that occur in source files are of
interest. Therefore, the same metric as in the original study by
\citet{karus2013} is used. The $LOC$ metric is constructed as the sum of
$loc\_fact$, $comments\_fact$, and $blanks\_fact$.

\paragraph{}
For the wavelet transform and analysis, I have used R Statistics Suite
with packages ``wavelets'', ``chron'' and ``zoo''. The Wavelets package
contains implementations of discrete wavelet transform functions, including
the Haar filter; Chron provides functions for working with chronological data
series; and Zoo eases indexing of and working with indexed data.

The Haar filter is used by \citet{karus2013} because of its simplicity and ease
of interpretation. As this study is a replication of \citeauthor{karus2013}'
study, the choice of using the Haar filter was made.

\paragraph{}
The R scripts used for the steps of wavelet transform, sequence identification,
and grouping, are based on the scripts created and used by
\citeauthor{karus2013} in the initial research. Small adjustments have been
made to make the scripts compatible with the data set.

\subsection{Wavelet transform}
The first step in the analysis of time series of software evolution is the
wavelet transform. During this step discrete wavelet transform using the Haar
filter is applied on a project's signal. The results comprise the
coefficients at each level of decomposition which are saved for further
analysis.

The coefficients are not the same as data points. The data points represent
monthly facts from a project, and the coefficients represent values found at a
certain level of decomposition from the wavelet transform. More specifically:
the coefficients are values resulted from shifting or scaling the signal. For
more details on wavelet transform and analysis see section
\ref{wavelet_analysis}.

\subsection{Wavelet analysis}
\subsubsection{Similar sequence identification}
The coefficients resulted from the wavelet transform are analysed to find
\emph{similar sequences}\rm. A sequence is defined as a subsequent series of
coefficients at a particular level of decomposition, with a sequence length
between 3 and 65 (inclusive) coefficients.

The sequence length thresholds were chosen by \citet{karus2013} in the original
study to distinguish a sequence from an ordinary set of coefficients if the
sequence is very short (length 1 or 2). Or, in case the sequence is very long,
the ability to distinguish the sequence from the complete signal, hence the
maximum length of 65.

\paragraph{}
In the search for similar sequences, the sequences of one project at a time are
analysed against sequences of other projects. A sequence is considered
'similar' if at least one other sequence was found of which the values of the
coefficients are equal with respect to an allowed deviation. The process of
finding similar sequences constructs ordered pairs of sequences from one
project to the other. The first element of the pair is the project of which the
sequence was first discovered, the second is the project of the similar
sequence. Similar sequences may be found within one project, but can also be
found across projects.

\subsubsection{Similar sequence grouping}
\label{def:pattern}
Not all of the sequences found in the previous step are of equal value. Many of
the sequences considered similar are contingent or represent trivialities. In
the sequence grouping step, the similar sequences are taken as input to find
'patterns'.

\paragraph{}
A sequence is considered a 'pattern' if it appears at least 3 times in the
sequence data. Each pattern is assigned an identification number to be used in
further analysis.

The sequences that are instances of the same pattern are grouped together and
added meta data, such as the number of occurrences, the list of projects
it occurs in, and whether this list of projects contains dead, alive, or both
dead and alive projects.

\paragraph{}
A distinction is made between the \emph{detection }\rm of a pattern and an
\emph{occurrence }\rm of a pattern. Although the difference is slight, the
distinction is essential for further analysis.

When \emph{detecting }\rm a pattern in project $A$, it means it was
encountered by analysing that particular project. Finding an \emph{occurrence
}\rm of a pattern means that the pattern was found by evaluating other project
$B$'s sequences against those from project $A$. A pattern is then recorded as a
tuple of a sequence detected in project $A$ and a set of at least 2 other
sequences being similar, occurring elsewhere.

\section{Pattern classification}
\label{section:patterns_dead}
Given the dead projects in the data set, all patterns detected in the dead
projects are selected. Patterns detected in dead projects can be hints of
evolutionary events that lead to the death of the project.

The patterns found in dead projects will not occur all at the same moment in
the lifetime of the projects. Therefore, different types of patterns for dead
projects are introduced:

\begin{description}
	\item[Type A.] Type A patterns strictly detected at the end of the evolution
		data of a dead project. More formaly, if a dead project $P$ starts at point
		$x$ and ends at point $y$, the pattern starts at $x'$ and ends at $y$.
	
	\item[Type B.] Type B patterns strictly \emph{not }\rm detected at the end of
		evolution data of a dead project. Such patterns may occur anywhere else in
		the lifetime of a dead project.
	
	\item[Type AB.] Type AB patterns are patterns that weakly satisfy the
		properties of both type A and B patterns. This category is needed to
		distinguish strict from weak patterns of type A.
\end{description}

\noindent
The above types partition the total set of encountered patterns into three
disjunctive sets.

\paragraph{}
In the search for patterns that are warning signs, the patterns of type A will
be the most interesting, because these patterns could indicate an evolutionary
event that has lead to the end of code evolution for the dead projects it
appears in.

\begin{comment}
- Execution of the research
- Phases, steps

This chapter reports on the execution of the research method as described in Chapter 3.

If the research has been divided into phases (e.g., using sub questions) the
phases are introduced, reported on and concluded individually. If needed this
Chapter could be split up to balance out the sizes of all Chapters.
An example Research Chapter is provided as Chapter 3 at Paul’s home
page\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/ReneWiegers.pdf}.
\end{comment}
