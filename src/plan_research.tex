\section{Research}

The main research question:
\begin{description}
	\item[RQ:] \emph{\researchQuestion}
\end{description}
Sub-questions:
\begin{description}
	\item[Q1:] \emph{\subQuestionOne}
	\item[Q2:] \emph{\subQuestionTwo}
\end{description}

\subsection{Theory}
The above research question is based on the observation that wavelet analysis is
used in other disciplines to analyse time series (e.g., economics and signal
processing). The idea behind a wavelet transform is that the same signal can
be sampled at different intervals giving a natural means for scaling it. In
case of software evolution, the signal can be any measurable property of an
evolving entity. Software repositories are a source of signals or time series
in software evolution. This allows the wavelet transform to be applied directly
for mining software repositories.

Wavelet analysis is analysis of signals (time series) by decomposing the signal
into wavelet coefficients and scaling coefficients based on wavelet functions
(also known as filters). The decomposition can be repeated on the scaling
coefficients until the number of resulting wavelet coefficients is smaller than
the filter length.

\subsection{Data}
Karus has used a data set of 27 OSS projects which form a representative
distribution of the OSS projects available at ohloh.net at the time of his
study. 18 of these projects were chosen randomly from Google Code Search from
various repositories containing team projects employing different source code
languages, team sizes, and project types. 15 of these projects are on-going and
3 have had no development activity since January 2009 (verified in January
2013). The alive projects have stayed alive for a minimum of 4 years.

In this Master's project, I will use a much larger data set (approx. 10
times) that still is a representative distribution of OSS projects at this time.
Using a larger data set will gather more statistical information to draw more
realistic conclusions from.

The evolution data of over 10,000 OSS projects is available at Ohloh.net. Not
all of these projects will be of equal use, but certainly a selection much
larger than the 27 projects used by Karus will be useful.

The data set will contain projects from different source code repositories,
different programming languages, project size, developers, and code activity.
The data set will be evaluated using techniques defined by Nagappan et al.
\cite{nagappan}.

\subsection{Metrics}
Karus conducted wavelet analysis in respect to two different time series
dimensions. The first is days since first commit and the second is cumulative
code churn.

Code churn is the sum of code added, modified, and deleted \cite{elbaum}. This
metric was chosen because of its popularity in project process measurement
frameworks \cite{karus2013}. Additionally, future cumulative code churn can be
estimated with reasonable accuracy based on project snapshots \cite{dumas}.
Therefore, cumulative code churn as development progress measure combines some
of the benefits of measuring progress in time spent and lines of code (LOC) of
final code produced.
\\

\noindent
Metrics related to developers:
\begin{itemize}
	\item average number of developers
	\item cumulative number of developers
	\item number of commits
	\item relative team size
\end{itemize}
Metrics related to code:
\begin{itemize}
	\item mean LOC added per commit
	\item mean LOC modified per commit
	\item mean LOC deleted per commit
	\item mean LOC churned per commit
\end{itemize}
Metrics related to project size:
\begin{itemize}
	\item LOC
	\item number of files
	\item relative progress by date
\end{itemize}

Karus used mean LOC and mean number of files to determine project size. We
believe this is a flawed number because a project usually starts small and
usually ends larger than it started. To determine a project's size at any point
in time, you need to take the LOC and number of files at that time and not the
average of what it has been since its existence.

\subsection{Method}
During this Master's project, I will replicate the study performed by Karus to
validate or refute his conclusions. I will elaborate on the kinds of events
that we are able to detect. Especially finding objective warning signs in
software evolution, and preferably finding these as early as possible.

The analysis and data preparation will be conducted in several steps:
\begin{enumerate}
	\item data aggregation
	\item discrete wavelet transform
	\item similar region detection
	\item similar region grouping
\end{enumerate}
The analysis and data aggregation will be performed using R Statistics Suite
with ``wavelets'', ``zoo'', and ``chron'' packages. Package ``wavelets''
provides discrete wavelet transform, package ``zoo'' time series methods, and
package ``chron'' extends support for date and time manipulations.

\paragraph{}
To find warning signs we first have to be able to detect events. The use
of wavelet analysis has been shown by Karus \cite{karus2013} to be a method
that can detect these events in a comparable manner. This I will replicate.
\\

Afterwards, we should dive into similar patterns that show a certain outcome.
For instance, the death of a project. If it shows that these patterns are a
result of similar activity or a lack of activity, which all lead to the death of
the project, then we might be able to explain that these specific patterns may
lead to the death of a project.
\\

\subsection{Validation}
For us to know what an objective warning sign is, we need to find common
patterns in the projects under study that had a negative effect on the project.
Negative effects include, but are not limited to, less developers activity,
lower code quality, lower progress speed, more defects, and maybe other effects
we don't know yet.
\\

Measuring developers activity can be done by looking at the number of
contributors and the individual number of commits. A sudden rise in bug reports
can be a sign of lower code quality.
\\

To validate whether we have discovered a pattern or event that lead to the end
of code evolution, we need to define 'end of code evolution' in a testable
manner. We could say that the evolution of code has ended if there was no
developers activity for, say, 12 months. If we find patterns that confirm this,
then we should validate with more data if the confirmation still holds.
\\

\noindent
\bfseries{Q1: \emph{\subQuestionOne}}\rm
\\

Karus had found that the wavelet analysis revealed 998 common patterns across
different projects. He concluded in a quick analysis that it was confirmed that
evolution patterns in software projects do express various warning signs leading
to or indicating the impending end of code evolution in the project
\cite{karus2013}.
\\

However, what patterns were actually detected? Are there more patterns to be
identified that are a result of other factors? To know what a pattern means in
the reality of the project, we have to take a look at what factors caused the
pattern. For instance, a sudden change in team size, progress, etc. Furthermore,
what was the impact of the pattern on the future of the project.
\\

Because the full data set will be too large to validate each case individually,
a subset needs to be taken. This subset will be a representative subset of the
full data set.
\\

\noindent
\bfseries{Q2: \emph{\subQuestionTwo}}\rm
\\

Wavelet analysis can be used to detect patterns that lead to end of code
evolution as found by Karus \cite{karus2013}. However, results of the wavelet
analysis may contain false-positives -- i.e., patterns that have no single
cause and do not neccessarily have negative effects on the project.
On the contrary, wavelet analysis may not detect events that had a negative
effect on the project.
\\

To answer this question we take two subsets of projects. For the false-positive
cases we take a subset of projects of which patterns were detected using the
wavelet analysis. We manually analyse these patterns on how they relate to the
events that occured in the project.
\\

For the false-negative cases we take a subset of projects with known events that
had a negative effect on the project (e.g., team change, project abandonment).
We can then verify if these events were detected using wavelet analysis and how
to recognize these.
