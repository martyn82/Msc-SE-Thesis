\chapter{Analysis and Conclusions}
\label{analysis}

\section{Evaluation}
\subsection{Wavelet analysis}
\label{section:gapless_wavelets}
In verification of Hypothesis \ref{hyp:subsequent_data}, the requirement of a
subsequent data series for wavelet analysis, a hypothetical signal was
constructed. Let the following set of pairs be the signal $S$ to be analysed,
where the first entry is the time value, and the second entry the LOC value:
$$S = \{(1,100), (2,150), (5,250)\}$$

\noindent
In signal $S$ the two pairs having first entry $3$ and $4$ are missing. The
implementation of discrete wavelet transform in the 'wavelets' R package
requires an indexed series as input wavelet. This means, that whether or not
the signal contains gaps, these gaps are naturally closed by indexing the
signal. After zero-based indexing, the signal $S$ will look as follows:
$$S' = \{(0,100), (1,150), (2,250)\}$$

\noindent
The wavelet transformation can perfectly be done using $S'$ as input
signal. The output series of coefficients are indistinguishable from other
signals without gaps. However, the patterns found will be invalid as it ignored
intermediate data during similarity analysis.

\paragraph{}
A further investigation was done on how to fix such gaps. The use of three
different methods of 'guessing' the data was explored and the results were the
following:
\begin{description}
	\item[last observation carried] \hfill \\[1em]
	$S_{1} = \{(1,100), (2,150), (3,150), (4,150), (5,250)\}$

	\item[mean] \hfill \\[1em]
	$S_{2} = \{(1,100), (2,150), (3,200), (4,200), (5,250)\}$
	
	\item[piecewise constant interpolation] \hfill \\[1em]
	$S_{3} = \{(1,100), (2,150), (3,150), (4,250), (5,250)\}$
	
	\item[linear interpolation] \hfill \\[1em]
	$S_{4} = \{(1,100), (2,150), (3,183), (4,216), (5,250)\}$
\end{description}

\paragraph{}
The method \emph{last observation carried }\rm results in a signal with a
same values between point 2 and 5, which shows a flat line when plotted in a
graph.

Filling the gaps with \emph{mean }\rm values, will also result in a signal with
same values. It is similar to last observation carried but using the mean is
excluding the two known observations, whereas last observation carried is
inclusive. Using mean, the number of second entries having same values
will therefore be smaller than using the last observation.

The \emph{piecewise constant }\rm interpolation results in a signal having two
smaller flat lines, because the repeating values are 'snapped' to the closest
neighbour.

Finally, the \emph{linear interpolation }\rm will result in a signal that
linearly flows towards the next known value. The advantage of this technique is
that there will not be a flat line in the signal, but the disadvantage is that
the numbers are new in the signal, possibly not even close to what was present
in the complete signal.

\paragraph{}
The flat lines introduced using the above techniques may be detected as no
change in LOC during that period. This is undesired as can be mistaken for a
pattern that stopped code evolution.

However, the signal resulted from linear interpolation could be mistaken for
growth or decay in LOC, depending on the direction.

\paragraph{}
The success of one of the above methods of closing a gap depends on the size of
the gap. The smaller the gap, the lesser impact it has on the reliability of
the detection of a pattern for that particluar signal. The larger the gap, the
more the impact on reliability. Either way, adding data will add noise.
However, in general having gaps in the data and close the gaps by adding data
negatively influences the reliability of detecting patterns. Therefore,
Hypothesis \ref{hyp:subsequent_data} is confirmed.

\subsection{Patterns}
\label{section:pattern_evaluation}
There are more than 100,000 times more similar sequences found in the
scale/filter coefficients than in the wavelet/shift coefficients. The large
difference in number of sequences found between the two types of coefficients
is due to the fact that the LOC metric is a cumulative metric. The typical
trend of a LOC signal is growth. This makes finding similar sequences using
shift coefficients (i.e., along the time axis) less likely.

\paragraph{}
No patterns were detected in shift coefficients. This can be explained by the
fact that the 16 similar sequences in shift coefficients are not similar within
the same group of sequences.

Additionally, the shift coefficients are incomparable to the filter coefficients
because they were found in a fundamentally different way of signal
transformation. Mixing both types of coefficients would neglect the way the
coefficients were found and invalidate the patterns comprising sequences of
both types of coefficients.

\paragraph{}
The patterns that were detected show strong similarity. The similarity was
demonstrated in Figure \ref{figure:patterns_plots} in section
\ref{section:seqs_patterns}. The figure presented four graphs of four distinct
patterns plotted as wavelets of the original signals. Each graph contains
between 141 and 216 wavelets. Because of the similarity, the shape of the
pattern is still quite clear.

\subsection{Survivability}
\label{section:kp_survival}
The type A patterns would be the best candidates for being 'warning signs' as
they are detected at the end of evolution in dead projects. To determine whether
the chances for a project to die increase when having a type A pattern, two
groups are made. One group consisting of projects having the pattern, and
a second group of equal size consisting of projects not having the pattern.

Both groups of 93 projects contain projects that have died. The second group -
the projects without the pattern - is representative to the complete data set.
Selecting projects and scoring for representativeness is done with the tool by
\citet{nagappan}. For each project of both groups it is recorded at what age the
project died, that is, if it died. For the others, the maximum age in the data
set is used. In Figure \ref{figure:kp_survival} the Kaplan-Meier estimation of
the survival function is shown for these groups.

\input{figures/kp_survival}

\noindent
The Kaplan-Meier estimation of survival function as depicted in Figure
\ref{figure:kp_survival} suggests that projects in group 1 - the projects
having the type A pattern - die earlier than projects in group 0 - the projects
without the pattern. This suggests that having found a type A pattern in a
project shortens the project's life. Therefore, Hypothesis
\ref{hyp:pattern_types} is confirmed; projects with a pattern occurring at the
end of evolution of a dead project (i.e., type A pattern) have a higher chance
of dying.

\section{Conclusions}
In this study, the patterns detected using wavelet analysis on LOC signals
comprise similar sequences of LOC series within and across projects. The
waveforms of these patterns vary in all kinds of shapes. The type A patterns
all show a stagnation of LOC changes, that is, very few changes relative to the
project size.

In general, the ability to detect a 'pattern' as being a group of similar
sequences within or across projects, depends on the similarity of the sequence
between other sequences within the same analysis. Therefore, it is important to
have a data set that is large enough and representative to the world of OSS
projects to be able to detect patterns that can be generalised
[\ref{itm:question_patterns}, \ref{itm:question_successfailure}].

\paragraph{}
Another aspect that contributes to the success or failure of wavelet analysis
for software evolution is that the input signal should be free of gaps. In
section \ref{section:gapless_wavelets} the confirmation of Hypothesis
\ref{hyp:subsequent_data} is argued.

In case an input LOC signal for analysis is not continuous within the start
and end boundaries of the signal, the wavelet analysis will detect patterns
that are not trustworthy. Wavelet analysis may find more false-positive
evolutionary events [\ref{itm:question_successfailure}].

\paragraph{}
In this study, I have shown that wavelet analysis is able to find patterns that
increase the chances of a project to end. As discussed in section
\ref{section:kp_survival}, it appears that there is a relation between a
pattern of type A and the death of a project, however, it is no causal
relation.

Furthermore, as the patterns found during analysis are subject to the data set
as a whole, I cannot conclude that wavelet analysis is able to find
\emph{objective }\rm warning signs in OSS projects
[\ref{itm:question_warningsigns}].

\section{Threats to validity}
\begin{comment}
* Is the Ohloh database a representation of the world of OSS projects?
* Is LOC as the sum of source lines of code, comments, and blank lines valid?
* The use of LOC as a measure of project evolution. Does it represent
activity/growth/whatever to say something about the project's status?
* A selection criterion for the projects was a continuous series of subsequent
monthly facts. Maybe the full series of evolution data of a project is needed in
order to find objective signs or to be able to compare different projects.
* Is 250 projects enough to detect patterns and generalise to the world of OSS
projects?
* Is monthly aggregated data fine-grained enough?

\end{comment}

\section{Future work}
% use other metrics
% even more data

\begin{comment}
- Analyse results
- Conclude and interpret results
- Answer hypotheses and research questions
- Threats to validity
- Discussion
- Future work
 
This chapter contains the analysis and interpretation of the results. The
research questions are answered as best as possible given the results that were
obtained. The analysis also discussed parts of the questions that were left
unanswered.

An important topic is the validity of the results.
What methods of validation were used?
Could the results be generalized to other cases?
What threats to validity can be identified?

There is room here to discuss the results of related scientific literature here
as well.
How do the results obtained here relate to other work, and what consequences are
there?
Did your approach work better or worse?
Did you learn anything new compared to the already existing body of knowledge?
Finally, what could you say in hindsight on the research approach by followed?
What could have done better?
What lessons have been learned?
What could other researchers use from your experience?

A separate section should be devoted to ‘future work,’ i.e., possible extension
points of your work that you have identified. Other researchers (or yourself)
could use those as a starting point.

Refer to Chapters 3.7 and 4 in this example thesis at Paul’s
homepage\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/ReneWiegers.pdf}.
\end{comment}
