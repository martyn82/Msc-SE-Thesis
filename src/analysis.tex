\chapter{Analysis and Conclusions}
\label{analysis}

\section{Evaluation}
\subsection{Patterns}
\label{section:pattern_evaluation}
There are more than 100,000 times more similar sequences found in the
scale/filter coefficients than in the wavelet/shift coefficients. The large
difference in number of sequences found between the two types of coefficients
is due to the fact that the LOC metric is a cumulative metric. The typical
trend of a LOC signal is growth. This makes finding similar sequences using
shift coefficients (i.e., along the time axis) less likely.

\paragraph{}
No patterns were detected in shift coefficients. This can be explained by the
fact that the 16 similar sequences in shift coefficients are not similar within
the same group of sequences.

Additionally, the shift coefficients are incomparable to the filter coefficients
because they were found in a fundamentally different way of signal
transformation. Mixing both types of coefficients would neglect the way the
coefficients were found and invalidate the patterns comprising sequences of
both types of coefficients.

\paragraph{}
The patterns that were detected show strong similarity. The similarity was
demonstrated in Figure \ref{figure:patterns_plots} in section
\ref{section:seqs_patterns}. The figure presented four graphs of four distinct
patterns plotted as wavelets of the original signals. Each graph contains
between 141 and 216 wavelets. Because of the similarity, the shape of the
pattern is still quite clear.

\subsection{Survivability}
\label{section:kp_survival}
The type A patterns would be the best candidates for being 'warning signs' as
they are detected at the end of evolution in dead projects. To determine
whether the chances for a project to die increase when having a type A pattern,
two groups are made. One group consisting of projects having the pattern, and
a second group of equal size consisting of projects not having the pattern.

Both groups of 93 projects contain alive and dead projects. The group with
projects without the pattern is representative to the data set as a whole.
Selecting projects and scoring for representativeness is done with the tool by
\citet{nagappan}. For each dead project in both groups it is recorded at what
age the project died. For the alive projects, the maximum age in the
data set is used. In Figure \ref{figure:kp_survival} the Kaplan-Meier
estimation of the survival function of the projects is shown for these groups.

\input{figures/kp_survival}

\noindent
The Kaplan-Meier estimation of survival function of the projects as depicted in
Figure \ref{figure:kp_survival} suggests that projects in group 1 - the
projects having the type A pattern - die earlier than projects in group 0 - the
projects without the pattern. This suggests that having found a type A pattern
in a project shortens the project's life. Projects with a pattern occurring at
the end of evolution of a dead project (i.e., type A pattern) have a higher
chance of dying.

\section{Conclusions}
\label{section:conclusions}
In this study, the patterns detected using wavelet analysis on LOC signals
comprise similar sequences of LOC series within and across projects. The
waveforms of these patterns vary in all kinds of shapes. The type A patterns
all show a stagnation of LOC changes, that is, very few changes relative to the
project size.

In general, the ability to detect a 'pattern' as being a group of similar
sequences within or across projects, depends on the similarity of the sequence
between other sequences within the same analysis. Therefore, it is important to
have a data set that is large enough and representative to the world of OSS
projects to be able to detect patterns that can be generalised
[\ref{itm:question_patterns}, \ref{itm:question_successfailure}].

\paragraph{}
Another aspect that contributes to the success or failure of wavelet analysis
for software evolution is that the input signal should be free of gaps. In
section \ref{section:gapless_wavelets} this is argued.

In the case an input LOC signal for analysis is not continuous within the start
and end boundaries of the signal, the wavelet analysis will detect patterns
that are not trustworthy. Wavelet analysis may find more false-positive
evolutionary events [\ref{itm:question_successfailure}].

\paragraph{}
In this study, I have shown that wavelet analysis is able to find patterns that
increase the chances of a project to end. As discussed in section
\ref{section:kp_survival}, it appears that there is a relation between a
pattern of type A and the death of a project, however, it is not found to be a
causal relation.

Furthermore, as the patterns found during analysis are subject to the data set
as a whole, I cannot conclude that wavelet analysis is able to find
\textit{objective} warning signs in OSS projects
[\ref{itm:question_warningsigns}].

\section{Threats to validity}
The following aspects were found which may lead to threats to the validity of
the results.

\begin{description}
	\item[Construct validity] \hfill
	
	\begin{description}
		\item[\rm{Missing historical data}] -- In the analysis of project's
			evolution data, only the data provided by the OhlohAnalytics tool
			\cite{ohlohanalytics, bruntink2013} was used.
			The data before the first data point in the set is not taken into account. It
			is possible that certain evolutionary events happened before the first point.
			These events were not detected as they lay beyond reach of this study.

		\item[\rm{Missing most recent data}] -- The data provided for the study has
			data points until June 2013. Therefore, not the most recent data of the
			projects is used.

		\item[\rm{LOC as activity indicator}] -- The use of LOC (defined as
			lines of code + comments + blanks) as a measure of project activity could be
			false. The LOC will not change between two months whenever the amount of code
			deleted is equal to the amount added. In that case, the churn would be twice
			the LOC added/deleted, but the LOC will stay the same.

			When patterns are detected that show a stagnation in LOC, the suggestion
			would be that the project's activity is decreasing. However, it might also
			be the case that by coincidence it seems activity is decreasing, but in
			reality lots of activity has been going on.

		\item[\rm{Data source}] -- Using only one data source (Ohloh) may
			influence the value of the metrics. Ohloh did the analysis of the project's
			source code repositories. Several studies have shown that the data provided
			by Ohloh needs a thorough examination and cleansing before it can be used
			\cite{bruntink2013, ohlohanalytics, bruntink2014}. The data was initially
			validated and cleansed, but to make it consistent. No actual verification on
			the correctness of the data was conducted.

	\begin{comment}
			Additionally, the data set of projects is a sample taken from all projects
			tracked by Ohloh. What is unclear is if the set of projects tracked by Ohloh
			is a representation of the world of OSS projects. Selecting a data set that
			is representative to the projects tracked by Ohloh does not automatically be
			representative to the world.

		\item[Sample size.] \hspace{1em} For this study, a set of 250 projects is
			used. Is this enough to generalise the results to the world of OSS projects.

		\item[Time resolution.] \hspace{1em} Is monthly aggregated data fine-grained
			enough?
	\end{comment}
	\end{description}

	\item[Internal validity] \hfill

	\begin{description}
		\item[\rm{Selection bias}] -- The selection criteria for the data was to have
			at least 12 data points. It turned out, no project in the set was younger
			than 14 months. It might be that these younger projects show significance in
			the ability to detect evolutionary events, possibly refute or confirm
			findings.
	\end{description}

	\item[External validity] \hfill

	\begin{description}
		\item[\rm{Replication}] -- It seems hard to precisely replicate the study
			as there are a multitude of possible configurations to be made that may, in
			the end, influence the results drastically. Such as, the definition of
			similarity between sequences (what deviation is allowed, what is the
			difference of coincidence and a reoccurring sequence), and the definition of
			a pattern (how many occurrences, minimum, and maximum length).
			
			Furthermore, there is the interpretation of what a 'warning sign' should
			look like on a pattern level.
	\end{description}
\end{description}

\section{Future work}
A next step in the research on the use of wavelet analysis for detecting
warning signs in software evolution would be to use a larger data set. It would
be interesting to know if the findings of the type A patterns will be
consistent. The whole usable data set for this study contains 5,986 projects
(section \ref{method:data}).

\paragraph{}
The analysis could be done on other signals. In this study, only LOC was used,
but many other signals can be constructed. LOC measures code activity to a
certain extent, but LOC churn could reflect code activity better. To use LOC
churn properly, the LOC modified fact is also needed.

Other metrics could be team size, developer's churn (the number of developers
added and removed from the team), contributor's code activity, bug reports,
defects per kLOC etc.

\begin{comment}
- Analyse results
- Conclude and interpret results
- Answer hypotheses and research questions
- Threats to validity
- Discussion
- Future work
 
This chapter contains the analysis and interpretation of the results. The
research questions are answered as best as possible given the results that were
obtained. The analysis also discussed parts of the questions that were left
unanswered.

An important topic is the validity of the results.
What methods of validation were used?
Could the results be generalized to other cases?
What threats to validity can be identified?

There is room here to discuss the results of related scientific literature here
as well.
How do the results obtained here relate to other work, and what consequences are
there?
Did your approach work better or worse?
Did you learn anything new compared to the already existing body of knowledge?
Finally, what could you say in hindsight on the research approach by followed?
What could have done better?
What lessons have been learned?
What could other researchers use from your experience?

A separate section should be devoted to ‘future work,’ i.e., possible extension
points of your work that you have identified. Other researchers (or yourself)
could use those as a starting point.

Refer to Chapters 3.7 and 4 in this example thesis at Paul’s
homepage\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/ReneWiegers.pdf}.
\end{comment}
