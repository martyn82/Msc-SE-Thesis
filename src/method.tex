\chapter{Research method}
\label{method}

\section{Research approach}
The research will be conducted in the following phases:
\begin{description}
	\item[Literature survey.] As part of the research plan, relevant literature
	will be studied in order to get familiar with the topics touched by this
	study. Important topics are the ecosystem of OSS organisation and community,
	and the success and survival factors of OSS projects. Literature regarding
	wavelets, (discrete) wavelet transform and analysis, and in particular the
	Haar wavelet will be studied to understand its workings.

	\item[Data selection.] In this phase, the OSS projects for the study will be
	selected. The evolution data of these OSS projects will be validated and
	cleansed to ensure all data for these projects is consistent.
	
	Another criterion for the data set is that the evolution series of the projects
	are continuous, i.e., that the data per project does not have gaps of missing
	data. The discussion for this requirement is done in section
	\ref{section:gapless_wavelets}.

	The data set for analysis should also be representative to the total set of
	OSS projects in order to be able to generalise findings from the
	projects within the data set to the world of OSS projects in general.
	
	\item[Wavelet analysis.] This phase is divided into two sub-steps: wavelet
	transform, and wavelet analysis.
	
	First, the evolution data of the selected projects will be transformed using
	discrete wavelet transform (DWT). The results of these transforms are kept for
	further analysis. The choice for a suitable signal will be made.
	
	Second, the results of the wavelet transform are being analysed to find
	patterns. A pattern is a sequence of transformed data that occurs multiple
	times. To be able to find patterns, a detailed definition of when a sequence
	is a pattern is needed. Choosing when a sequence becomes a pattern in terms of
	number of occurrences might influence the number of patterns found and
	possibly influence the types of patterns wavelet analysis is able to detect.
	
	\item[Pattern identification.] The patterns found in the Wavelet analysis step
	will be further analysed to find patterns that are 'warning signs'. This is a
	pattern that leads to the 'end of code evolution' for a project (recall
	\ref{itm:question_warningsigns}). The validation needed in this step is
	described in section \ref{method:validation}.
	
	\item[Analysis and conclusions.] The final phase in the study is the analysis
	of the results to come to conclusions and answers to the research questions
	(section \ref{questions}).
\end{description}

\section{Continuous series}
\label{section:gapless_wavelets}
In verification of the requirement of a continuous series for wavelet analysis,
a hypothetical signal was constructed. Let the following set of pairs be the
signal $S$ to be analysed, where the first entry is the time value, and the
second entry the LOC value:
$$S = \{(1,100), (2,150), (5,250)\}$$

\noindent
In signal $S$ the two pairs having first entry $3$ and $4$ are missing. The
implementation of discrete wavelet transform in the 'Wavelets' R package
requires an indexed series as input wavelet. This means that whether or not
the signal contains gaps, these gaps are naturally closed by indexing the
signal. After zero-based indexing, the signal $S$ will look as follows:
$$S' = \{(0,100), (1,150), (2,250)\}$$

\noindent
The wavelet transformation can perfectly be done using $S'$ as input
signal. The output series of coefficients are indistinguishable from other
signals without gaps. However, the patterns found will be invalid as it ignored
intermediate data during similarity analysis.

\paragraph{}
A further investigation was done on how to fix such gaps. The use of four
different methods of 'guessing' the data was explored and the results were the
following:
\begin{description}
	\item[last observation carried] \hfill \\[1em]
	$S_{1} = \{(1,100), (2,150), (3,150), (4,150), (5,250)\}$

	\item[mean] \hfill \\[1em]
	$S_{2} = \{(1,100), (2,150), (3,200), (4,200), (5,250)\}$
	
	\item[piecewise constant interpolation] \hfill \\[1em]
	$S_{3} = \{(1,100), (2,150), (3,150), (4,250), (5,250)\}$
	
	\item[linear interpolation] \hfill \\[1em]
	$S_{4} = \{(1,100), (2,150), (3,183), (4,216), (5,250)\}$
\end{description}

\paragraph{}
The method \textit{last observation carried} results in a signal with
same values between point 2 and 5, which shows a flat line when plotted in a
graph.

\paragraph{}
Filling the gaps with \textit{mean} values, will also result in a signal with
same values. It is similar to last observation carried but with one difference.
The mean method is exclusive the two known observations, whereas last
observation carried is inclusive. Using mean, the number of second entries
having same values will therefore be 2 observations lesser than using the last
observation.

\paragraph{}
The \textit{piecewise constant} interpolation results in a signal having two
shorter flat lines. The repeating values are 'snapped' to the closest
neighbour.

\paragraph{}
Finally, the \textit{linear interpolation} will result in a signal that
linearly flows towards the next known value. The advantage of this technique is
that there will not be a flat line in the resulting signal. However, the
disadvantage is that resulting values possibly are not even close to what was
present in the complete signal.

\paragraph{}
The flat lines introduced using the above techniques may be detected as no
change in LOC during that period. This is undesired as can be mistaken for a
pattern that stopped code evolution.

Moreover, the signal resulted from linear interpolation could be mistaken for
growth or decay in LOC, depending on the direction.

\paragraph{}
The success of one of the above methods of closing a gap depends on the size of
the gap. The smaller the gap, the lesser impact it has on the reliability of
the detection of a pattern for that particular signal. The larger the gap, the
more the impact on reliability. Either way, adding data will add noise and, in
general, having gaps in the evolution data and closing the gaps by adding data
will negatively influence the reliability of detecting patterns.

\section{Validation}
\subsection{Dead projects}
\label{method:validation}
In order to validate the patterns and in particular to know if a pattern
represents an evolutionary event leading to the end of code evolution, dead
projects have to be identified.\\

\noindent
\label{def:dead}
A project is considered 'dead' if it complies to the following properties:
\begin{enumerate}
	\item the project has had 0 contributors in code for the last 12 months;
	\item the project has had 0 'lines of code' (LOC) churn\footnote{LOC churn:
	the sum of LOC added and LOC deleted \cite{elbaum}.} for the last 12 months.
\end{enumerate}

\noindent
The above properties combined specify that the project has had no code activity
for the past year, meaning the code evolution of the project has stopped. If the
LOC churn of a series of data points equals 0 (zero), this implies that the
difference in 'lines of code' (LOC) over that same series also equals 0 (i.e.,
there was no growth or decay in code size).

According to this definition, changes in documentation, wiki pages, external
library updates, and other non-code changes are still allowed for dead
projects.

To compare dead and alive projects from the data set, the above definition can
be used to identify the projects from the data set that are dead. The projects
that are not 'dead' by this definition are considered to be 'alive'.

\paragraph{}
To be sure that the projects from our data set that comply to the definition of
dead are still dead beyond the data set, manual validation of these findings
have to be done. In verifying if a project is dead, I will manually consult the
project's website, its source code repository, and possibly other sources to
verify the properties of a 'dead' project.

\subsection{Warning signs}
To recognise patterns that are warning signs, a selection of patterns occurring
in dead projects will be made for further analysis. The best candidates for
patterns being a warning sign are patterns occurring near the end of the
evolution of a dead project.

Knowing these patterns, selecting the projects having these patterns will be
done to find out if these projects have a higher chance of dying than projects
that do not show such pattern. An analysis of survival will be done to evaluate
this.

\begin{comment}
- The plan
- Methodology / method per question
- Hypotheses
- Validation

This section describes the methods used to answer the research questions. A
good structure of this section often follows the sub questions by providing a
method for each.

The research method can be based on the “Scientific method”, but more creative
solutions could be defined as well. In any case, the method needs a thorough
motivation grounded in theory in order to be acceptable.

As part of the method a number of hypotheses are described. These hypotheses
will be tested by the research, using the methods described here.

An important part of this section is validation. How will you evaluate and
validate the outcomes of the research? You can look at Paul Klint’s homepage
for examples of this section as
well\footnote{http://homepages.cwi.nl/~paulk/thesesMasterSoftwareEngineering/2006/RichardKettelerij.pdf}.
\end{comment}